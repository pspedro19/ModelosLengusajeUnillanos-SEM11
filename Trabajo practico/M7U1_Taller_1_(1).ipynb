{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dbcf0568",
      "metadata": {
        "id": "dbcf0568"
      },
      "source": [
        "# **Taller 1: Fundamentos de los LLM**\n",
        "---\n",
        "\n",
        "En este taller se evaluarán las habilidades aprendidas en las herramientas para grandes modelos del lenguaje (LLMs - Large Language Models) en _Python_. En este caso, usted deberá construir una sesión de chat a partir de un modelo generativo pre-entrenado de Hugging Face.\n",
        "\n",
        "Comenzamos instalando las librerías necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-63Ib7khKN1A",
      "metadata": {
        "id": "-63Ib7khKN1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b97494-aff7-44a3-8d22-5d43f2cd86e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rlxcrypt\n",
            "  Downloading rlxcrypt-0.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (297 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/297.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/297.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.9/297.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting imphook (from rlxcrypt)\n",
            "  Downloading imphook-1.0.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from rlxcrypt) (3.0.5)\n",
            "Collecting pycryptodome (from rlxcrypt)\n",
            "  Downloading pycryptodome-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from rlxcrypt) (41.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->rlxcrypt) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->rlxcrypt) (2.21)\n",
            "Building wheels for collected packages: imphook\n",
            "  Building wheel for imphook (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imphook: filename=imphook-1.0-py3-none-any.whl size=9421 sha256=9facd3488158c5225e830a87b3716c68def7c1a00fbdfc015d9aad279e9de195\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/e2/a4/fcb3817d09a2eb047b2b08eb58e7d9140041b0f3f415eb1256\n",
            "Successfully built imphook\n",
            "Installing collected packages: imphook, pycryptodome, rlxcrypt\n",
            "Successfully installed imphook-1.0 pycryptodome-3.19.0 rlxcrypt-0.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install rlxcrypt\n",
        "!wget --no-cache -O session.pye -q https://raw.githubusercontent.com/JuezUN/INGInious/master/external%20libs/session.pye"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab27ce3f",
      "metadata": {
        "id": "ab27ce3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b256c8fa-6988-4d96-a203-359ccabbabdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers\n",
            "  Downloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (211.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers[torch])\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[torch])\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: sentencepiece, safetensors, sacremoses, huggingface-hub, xformers, tokenizers, accelerate, transformers\n",
            "Successfully installed accelerate-0.24.1 huggingface-hub-0.17.3 sacremoses-0.1.1 safetensors-0.4.0 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.35.0 xformers-0.0.22.post7\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch] xformers sentencepiece sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C-Hk4FwUKOKL",
      "metadata": {
        "id": "C-Hk4FwUKOKL"
      },
      "source": [
        "Nos conectamos con UNCode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84QVxmWcKSyL",
      "metadata": {
        "id": "84QVxmWcKSyL"
      },
      "outputs": [],
      "source": [
        "import rlxcrypt\n",
        "import session\n",
        "\n",
        "grader = session.LoginSequence(\"DDACGMDL-GroupMLDS-7-2023-1@taller_1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eadf4790",
      "metadata": {
        "id": "eadf4790"
      },
      "source": [
        "Importamos las librerías:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66602eb6",
      "metadata": {
        "id": "66602eb6"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, set_seed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e597ee37",
      "metadata": {
        "id": "e597ee37"
      },
      "source": [
        "## **1. Carga del Tokenizador**\n",
        "---\n",
        "\n",
        "En este punto deberá cargar un tokenizador pre-entrenado para el modelo GPT2. Como se trata de un modelo muy general, no es necesario que realice una búsqueda del modelo en la página de Hugging Face. Puede cargarlo directamente como `\"gpt2\"`.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1UzZNT2Brf2dcH7dT9p21M5CflAO_ctF-\" width=\"80%\"></center>\n",
        "\n",
        "<center>GIF tomado de <a href = \"https://jalammar.github.io/illustrated-gpt2/\"> Jay Alammar </a></center>\n",
        "\n",
        "Para esto debe implementar la función `get_tokenizer` la cual debe retornar el tokenizador.\n",
        "\n",
        "**Parámetros**\n",
        "\n",
        "Esta función no recibe parámetros.\n",
        "\n",
        "**Retorna**\n",
        "\n",
        "- `tokenizer`: el tokenizador de GPT2 pre-entrenado."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "689b5d12",
      "metadata": {
        "id": "689b5d12"
      },
      "source": [
        "<details>\n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Pistas</b></font>\n",
        "</summary>\n",
        "\n",
        "- Recuerde que puede usar el método de clase `from_pretrained` para descargar tanto modelos como tokenizadores.\n",
        "- No es necesario hacer la búsqueda de modelo en la página de huggingface. Con usar `\"gpt2\"` es suficiente.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e99d814",
      "metadata": {
        "id": "8e99d814"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN CALIFICADA get_tokenizer:\n",
        "\n",
        "\n",
        "def get_tokenizer():\n",
        "    ### ESCRIBA SU CÓDIGO AQUÍ ###\n",
        "\n",
        "    tokenizer = ...\n",
        "    return tokenizer\n",
        "    ### FIN DEL CÓDIGO ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92494906",
      "metadata": {
        "id": "92494906"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "tokenizer = get_tokenizer()\n",
        "display(tokenizer.name_or_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd2f6476",
      "metadata": {
        "id": "fd2f6476"
      },
      "source": [
        "**Salida esperada:**\n",
        "\n",
        "- En este caso debe obtener el nombre del tokenizador:\n",
        "\n",
        "```python\n",
        "❱ display(tokenizer.name_or_path)\n",
        "'gpt2'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e309eb86",
      "metadata": {
        "id": "e309eb86"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "display(tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11937fae",
      "metadata": {
        "id": "11937fae"
      },
      "source": [
        "**Salida esperada:**\n",
        "\n",
        "- En este caso debe obtener el tamaño del vocabulario del tokenizador:\n",
        "\n",
        "```python\n",
        "❱ display(tokenizer.vocab_size)\n",
        "50257\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mCi2TPsQM2Vw",
      "metadata": {
        "id": "mCi2TPsQM2Vw"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2kSmTsSqvQ4",
      "metadata": {
        "id": "f2kSmTsSqvQ4"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 1_1\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99fd47b1",
      "metadata": {
        "id": "99fd47b1"
      },
      "source": [
        "## **2. Extracción de tokens**\n",
        "---\n",
        "\n",
        "En este punto deberá extraer un vector numérico en `pytorch` a partir de un texto y un tokenizador.\n",
        "\n",
        "Para esto, deberá implementar la función `extract_tokens` la cual toma como entrada una cadena de texto y un tokenizador pre-entrenado para generar un vector numérico con las codificaciones.\n",
        "\n",
        "**Parámetros**\n",
        "\n",
        "- `text`: texto de entrada.\n",
        "- `tokenizer`: tokenizador a aplicar.\n",
        "\n",
        "**Retorna**\n",
        "\n",
        "- `tokens`: vector numérico en `pytorch`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c03e6f70",
      "metadata": {
        "id": "c03e6f70"
      },
      "source": [
        "<details>\n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Pistas</b></font>\n",
        "</summary>\n",
        "\n",
        "- El tokenizador puede ser usado como cualquier función de _Python_. Es decir, no necesita usar ningún método especial.\n",
        "- Recuerde indicar que los tokens se codificarán en el formato de `torch` al especificarlo en el parámetro `return_tensors`.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc378cf1",
      "metadata": {
        "id": "dc378cf1"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN CALIFICADA extract_tokens:\n",
        "\n",
        "\n",
        "def extract_tokens(text, tokenizer):\n",
        "    ### ESCRIBA SU CÓDIGO AQUÍ ###\n",
        "\n",
        "    tokens = ...\n",
        "    return tokens\n",
        "    ### FIN DEL CÓDIGO ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b920fc88",
      "metadata": {
        "id": "b920fc88"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "tokenizer = get_tokenizer()\n",
        "tokens = extract_tokens(\"hello world\", tokenizer)\n",
        "display(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "976ceeac",
      "metadata": {
        "id": "976ceeac"
      },
      "source": [
        "**Salida esperada:**\n",
        "\n",
        "- En este caso debe obtener las entradas del modelo:\n",
        "\n",
        "```python\n",
        "❱ display(tokens)\n",
        "{'input_ids': tensor([[31373,   995]]), 'attention_mask': tensor([[1, 1]])}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184ca330",
      "metadata": {
        "id": "184ca330"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "tokenizer = get_tokenizer()\n",
        "tokens = extract_tokens(\"basic model example\", tokenizer)\n",
        "display(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1db902fa",
      "metadata": {
        "id": "1db902fa"
      },
      "source": [
        "**Salida esperada:**\n",
        "\n",
        "- En este caso debe obtener las entradas del modelo:\n",
        "\n",
        "```python\n",
        "❱ display(tokens)\n",
        "{'input_ids': tensor([[35487,  2746,  1672]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5PtfWZUfM-rl",
      "metadata": {
        "id": "5PtfWZUfM-rl"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bpm4xHRurLzr",
      "metadata": {
        "id": "bpm4xHRurLzr"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 2_1\", globals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N3_bW8hg6fWt",
      "metadata": {
        "id": "N3_bW8hg6fWt"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 2_2\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62a09d41",
      "metadata": {
        "id": "62a09d41"
      },
      "source": [
        "## **3. Carga de Modelo**\n",
        "---\n",
        "\n",
        "En este punto deberá cargar un modelo pre-entrenado de GPT2 para generación de texto. Esto lo debe hacer por medio de la clase `GPT2LMHeadModel` que está pensada para utilizar los modelos como secuencia a secuencia.\n",
        "\n",
        "Para esto debe implementar la función `get_model`, la cual debe retornar un modelo pre-entrenado de GPT2. Al igual que con el tokenizador, no es necesario buscar un modelo, puede cargar los el modelo como `\"gpt2\"`.\n",
        "\n",
        "**Parámetros**\n",
        "\n",
        "Esta función no tiene parámetros.\n",
        "\n",
        "**Retorna**\n",
        "\n",
        "- `model`: modelo de GPT2 pre-entrenado."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85066936",
      "metadata": {
        "id": "85066936"
      },
      "source": [
        "<details>\n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Pistas</b></font>\n",
        "</summary>\n",
        "\n",
        "- Recuerde utilizar el método de clase `from_pretrained` para cargar el modelo.\n",
        "- Recuerde que no debe buscar un modelo, puede usar directamente `\"gpt2\"` como modelo pre-entrenado.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd6be169",
      "metadata": {
        "id": "cd6be169"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN CALIFICADA get_model:\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    ### ESCRIBA SU CÓDIGO AQUÍ ###\n",
        "\n",
        "    model = ...\n",
        "    return model\n",
        "    ### FIN DEL CÓDIGO ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36d043f4",
      "metadata": {
        "id": "36d043f4"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "model = get_model()\n",
        "display(model.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd65c485",
      "metadata": {
        "id": "bd65c485"
      },
      "source": [
        "**Salida esperada:**\n",
        "\n",
        "- En este caso debe obtener la descripción del modelo.\n",
        "\n",
        "```python\n",
        "❱ display(model.config)\n",
        "GPT2Config {\n",
        "  \"_name_or_path\": \"gpt2\",\n",
        "  \"activation_function\": \"gelu_new\",\n",
        "  \"architectures\": [\n",
        "    \"GPT2LMHeadModel\"\n",
        "  ],\n",
        "  \"attn_pdrop\": 0.1,\n",
        "  \"bos_token_id\": 50256,\n",
        "  \"embd_pdrop\": 0.1,\n",
        "  \"eos_token_id\": 50256,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"layer_norm_epsilon\": 1e-05,\n",
        "  \"model_type\": \"gpt2\",\n",
        "  \"n_ctx\": 1024,\n",
        "  \"n_embd\": 768,\n",
        "  \"n_head\": 12,\n",
        "  \"n_inner\": null,\n",
        "  \"n_layer\": 12,\n",
        "  \"n_positions\": 1024,\n",
        "  \"reorder_and_upcast_attn\": false,\n",
        "  \"resid_pdrop\": 0.1,\n",
        "  \"scale_attn_by_inverse_layer_idx\": false,\n",
        "  \"scale_attn_weights\": true,\n",
        "  \"summary_activation\": null,\n",
        "  \"summary_first_dropout\": 0.1,\n",
        "  \"summary_proj_to_labels\": true,\n",
        "  \"summary_type\": \"cls_index\",\n",
        "  \"summary_use_proj\": true,\n",
        "  \"task_specific_params\": {\n",
        "    \"text-generation\": {\n",
        "      \"do_sample\": true,\n",
        "      \"max_length\": 50\n",
        "    }\n",
        "  },\n",
        "  \"transformers_version\": \"4.30.2\",\n",
        "  \"use_cache\": true,\n",
        "  \"vocab_size\": 50257\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qRoapK75M_7m",
      "metadata": {
        "id": "qRoapK75M_7m"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zqGr2hI_8CUY",
      "metadata": {
        "id": "zqGr2hI_8CUY"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 3_1\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15d86526",
      "metadata": {
        "id": "15d86526"
      },
      "source": [
        "## **4. Inferencia**\n",
        "---\n",
        "\n",
        "En este punto deberá implementar una función para obtener la generación de texto del modelo. Para ello partirá de un texto, el tokenizador, el modelo pre-entrenado y deberá retornar el texto generado. La generación de texto la puede realizar con el método `generate` del modelo, el cual recibe las entradas del modelo y algunos hiperparámetros como `max_new_tokens` para acotar el número máximo de tokens a generar, `temperature` para controlar el determinismo y `repetition_penalty` para condenar la repetitividad.\n",
        "\n",
        "Para esto debe implementar la función `inference` la cual toma como entrada el texto inicial, el tokenizador, el modelo y algunos hiperparámetros de generación, y debe retornar el texto generado.\n",
        "\n",
        "**Parametros**\n",
        "\n",
        "- `text`: string con el texto inicial sobre el que se generará.\n",
        "- `tokenizer`: tokenizador de GPT2.\n",
        "- `model`: modelo GPT2.\n",
        "- `max_new_tokens`: número máximo de tokens a generar.\n",
        "- `temperature`: temperatura de generación.\n",
        "- `repetition_penalty`: penalidad de repetición de palabras.\n",
        "\n",
        "**Retorna**\n",
        "\n",
        "- `generated_text`: texto generado."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e479299e",
      "metadata": {
        "id": "e479299e"
      },
      "source": [
        "<details>\n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Pistas</b></font>\n",
        "</summary>\n",
        "\n",
        "- Puede utilizar las funciones que implementó en puntos anteriores para obtener la respuesta.\n",
        "- El texto generado por el modelo viene codificado de forma numérica. Para obtener el resultado final puede utilizar el método `batch_decode`.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b84e6801",
      "metadata": {
        "id": "b84e6801"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN CALIFICADA inference:\n",
        "\n",
        "\n",
        "def inference(text, tokenizer, model, max_new_tokens, temperature, repetition_penalty):\n",
        "    ### ESCRIBA SU CÓDIGO AQUÍ ###\n",
        "\n",
        "    generated_text = ...\n",
        "    return generated_text\n",
        "    ### FIN DEL CÓDIGO ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3492a03c",
      "metadata": {
        "id": "3492a03c"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "set_seed(0)\n",
        "tokenizer = get_tokenizer()\n",
        "model = get_model()\n",
        "generated_text = inference(\n",
        "    text=\"My name is\",\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    max_new_tokens=20,\n",
        "    temperature=0.1,\n",
        "    repetition_penalty=2.0,\n",
        ")\n",
        "display(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b59c53e",
      "metadata": {
        "id": "2b59c53e"
      },
      "source": [
        "**Salida esperada:**\n",
        "\n",
        "- En este caso debe obtener el siguiente texto generado:\n",
        "\n",
        "```python\n",
        "❱ display(generated_text)\n",
        "[\"My name is John. I'm a man of God, and you know what? You're not going to believe\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d-oAMF6sNA80",
      "metadata": {
        "id": "d-oAMF6sNA80"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rwzoUF30D-Qc",
      "metadata": {
        "id": "rwzoUF30D-Qc"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 4_1\", globals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nX-fCQWsJ58h",
      "metadata": {
        "id": "nX-fCQWsJ58h"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 4_2\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "668510b6",
      "metadata": {
        "id": "668510b6"
      },
      "source": [
        "## **5. Sesión de Chat**\n",
        "---\n",
        "\n",
        "En este punto debe implementar una sesión de chat a partir de una plantilla. La idea es utilizar la parte generativa del modelo para completar conversaciones.\n",
        "\n",
        "En este caso la conversación lucirá de la siguiente forma:\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "The following text is a conversation between a man and a robot:\n",
        "\n",
        "[user]\n",
        "Hello.\n",
        "\n",
        "[robot]\n",
        "Greetings, how may I assist you?\n",
        "\n",
        "[user]\n",
        "Should I work as a programmer?\n",
        "\n",
        "[robot]\n",
        "I'm not sure what to do with this information... but it's nice that we can talk about things like programming languages in the future!\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "Para esto, partiremos de la siguiente plantilla:\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "The following text is a conversation between a man and a robot:\n",
        "\n",
        "{history}\n",
        "\n",
        "[user]\n",
        "{msg}\n",
        "\n",
        "[robot]\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "En `history` tendremos el histórico de la conversación, `msg` contendrá el mensaje a enviar. Para este punto debe utilizar la plantilla de entrada y los formatos para generar una respuesta con el modelo. Dicha respuesta debe incluir únicamente el mensaje generado.\n",
        "\n",
        "Para esto, debe implementar la función `chat_session`.\n",
        "\n",
        "**Parámetros**\n",
        "\n",
        "- `template`: plantilla de chat.\n",
        "- `history`: histórico de mensajes.\n",
        "- `message`: mensaje a enviar.\n",
        "- `tokenizer`: tokenizador de GPT2.\n",
        "- `model`: modelo GPT2 pre-entrenado.\n",
        "- `max_new_tokens`: número máximo de tokens a generar.\n",
        "- `temperature`: temperatura de generación.\n",
        "- `repetition_penalty`: penalidad de repetición de palabras.\n",
        "\n",
        "**Retorna**\n",
        "\n",
        "- `generated_text`: mensaje generado."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a971122",
      "metadata": {
        "id": "0a971122"
      },
      "source": [
        "<details>\n",
        "<summary>\n",
        "    <font size=\"3\" color=\"darkgreen\"><b>Pistas</b></font>\n",
        "</summary>\n",
        "\n",
        "- Puede revisar el método `format` de los strings en _Python_ para dar formato a la plantilla.\n",
        "- Recuerde cortar el texto generado para incluir únicamente la parte generada.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8f92874",
      "metadata": {
        "id": "b8f92874"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN CALIFICADA chat_session:\n",
        "\n",
        "def chat_session(\n",
        "    template,\n",
        "    history,\n",
        "    message,\n",
        "    tokenizer,\n",
        "    model,\n",
        "    max_new_tokens,\n",
        "    temperature,\n",
        "    repetition_penalty,\n",
        "):\n",
        "    ### ESCRIBA SU CÓDIGO AQUÍ ###\n",
        "\n",
        "    generated_text = ...\n",
        "    return generated_text\n",
        "    ### FIN DEL CÓDIGO ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20784645",
      "metadata": {
        "id": "20784645"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "set_seed(0)\n",
        "template = \"\"\"\n",
        "The following text is a conversation between a man and a robot:\n",
        "{history}\n",
        "[user]\n",
        "{message}\n",
        "[robot]\n",
        "\"\"\"\n",
        "history = \"\"\n",
        "message = \"Hello.\"\n",
        "tokenizer = get_tokenizer()\n",
        "model = get_model()\n",
        "generated_text = chat_session(\n",
        "    template=template,\n",
        "    history=history,\n",
        "    message=message,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    max_new_tokens=15,\n",
        "    temperature=0.1,\n",
        "    repetition_penalty=2.0,\n",
        ")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1edc3aa6",
      "metadata": {
        "id": "1edc3aa6"
      },
      "source": [
        "**Salida esperada:**\n",
        "\n",
        "- En este caso debe obtener el mensaje del robot:\n",
        "\n",
        "```python\n",
        "❱ print(generated_text)\n",
        "\n",
        ", I'm an engineer at the company that makes robots for your business!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79960af1",
      "metadata": {
        "id": "79960af1"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "set_seed(0)\n",
        "template = \"\"\"\n",
        "The following text is a conversation between a man and a robot:\n",
        "{history}\n",
        "[user]\n",
        "{message}\n",
        "[robot]\n",
        "\"\"\"\n",
        "history = \"\"\"\n",
        "[user]\n",
        "Hello.\n",
        "[robot]\n",
        "Greetings.\n",
        "\"\"\"\n",
        "message = \"There's a problem with my card payment\"\n",
        "tokenizer = get_tokenizer()\n",
        "model = get_model()\n",
        "generated_text = chat_session(\n",
        "    template=template,\n",
        "    history=history,\n",
        "    message=message,\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    max_new_tokens=14,\n",
        "    temperature=1.0,\n",
        "    repetition_penalty=2.0,\n",
        ")\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d469624",
      "metadata": {
        "id": "2d469624"
      },
      "source": [
        "**Salida esperada:**\n",
        "\n",
        "- En este caso debe obtener el mensaje del robot:\n",
        "\n",
        "```python\n",
        "❱ print(generated_text)\n",
        "\n",
        ", but I'm not going to pay it back for you guys!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WvQqn4gaNCAh",
      "metadata": {
        "id": "WvQqn4gaNCAh"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nREcBMkI4Z3N",
      "metadata": {
        "id": "nREcBMkI4Z3N"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 5_1\", globals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SiDHAzgUTF7a",
      "metadata": {
        "id": "SiDHAzgUTF7a"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 5_2\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SoHNDOzkJoSv",
      "metadata": {
        "id": "SoHNDOzkJoSv"
      },
      "source": [
        "## **Evaluación**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HEBjfhJGJ13a",
      "metadata": {
        "id": "HEBjfhJGJ13a"
      },
      "outputs": [],
      "source": [
        "grader.submit_task(globals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ticqp2LG78yK",
      "metadata": {
        "id": "ticqp2LG78yK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "title,-all",
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}