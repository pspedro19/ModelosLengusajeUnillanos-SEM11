{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "02bcd83c",
      "metadata": {
        "id": "02bcd83c"
      },
      "source": [
        "# OpenAI\n",
        "---\n",
        "\n",
        "[OpenAI](https://openai.com/) es una empresa de investigación y desarrollo de inteligencia artificial (IA) con sede en San Francisco, California. Fue fundada en 2015 con el objetivo de promover y desarrollar IA amigable y beneficios para la humanidad. OpenAI se dedica a avanzar en el campo de la IA y realizar investigaciones novedosas en áreas como el procesamiento del lenguaje natural, la visión por computadora y la robótica.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1wNUZyK7x3nkM-i-Q-AYPNqldJrNiKAx8\" width=\"50%\"> </center>\n",
        "\n",
        "OpenAI ha creado y publicado una serie de modelos y herramientas de IA de alta importancia, incluido el modelo de lenguaje GPT (Generative Pre-trained Transformer) y su sucesor GPT-3. Estos modelos han demostrado una capacidad impresionante para generar texto coherente y comprensible, y se han utilizado en una amplia gama de aplicaciones, desde chatbots y asistentes virtuales hasta la redacción automática y la traducción automática.\n",
        "\n",
        "Actualmente, podemos utilizar varios de los modelos que ha implementado OpenAI. Más precisamente, con la librería `openai` en _Python_ podemos realizar inferencia de forma sencilla, veamos la instalación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93521b5a",
      "metadata": {
        "title": "[code]",
        "id": "93521b5a"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b63c774d",
      "metadata": {
        "id": "b63c774d"
      },
      "source": [
        "Adicional a esto, vamos a importar algunas utilidades generales:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6e47b20",
      "metadata": {
        "title": "[code]",
        "id": "f6e47b20"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01bb4480",
      "metadata": {
        "id": "01bb4480"
      },
      "source": [
        "## **1. Autenticación**\n",
        "---\n",
        "\n",
        "Para poder utilizar los modelos de OpenAI debemos [crear una cuenta](https://platform.openai.com/signup) para uso con el API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3183aad",
      "metadata": {
        "id": "e3183aad",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##**Ejecute esta celda para ver el video.**\n",
        "from IPython.display import IFrame\n",
        "IFrame(\n",
        "        src=\"https://drive.google.com/file/d/1WS56kzn6SyfCizDGA4i6Fpws5Cc6HH22/preview\",\n",
        "        width=\"768px\",\n",
        "        height=\"432px\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38406579",
      "metadata": {
        "id": "38406579"
      },
      "source": [
        "> **Nota**: para esto debe introducir una tarjeta bancaria válida. A nivel personal los gastos son mínimos pero es importante que tenga cuidado al momento de manejar los datos de acceso.\n",
        "\n",
        "Adicional a esto, para poder conectarnos con los servidores de OpenAI, necesitamos los siguientes dos elementos:\n",
        "\n",
        "- **Organization ID**: es un identificador personal (usuario) o de la empresa correspondiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd2aa101",
      "metadata": {
        "id": "cd2aa101",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##**Ejecute esta celda para ver el video.**\n",
        "from IPython.display import IFrame\n",
        "IFrame(\n",
        "        src=\"https://drive.google.com/file/d/12muJDDxJlSTW-UWAniDn-yC3lj6Qfjqy/preview\",\n",
        "        width=\"768px\",\n",
        "        height=\"432px\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a43e709",
      "metadata": {
        "id": "9a43e709"
      },
      "source": [
        "Debe agregar el id de la organización en la siguiente variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ffdcf9",
      "metadata": {
        "title": "[code]",
        "id": "95ffdcf9"
      },
      "outputs": [],
      "source": [
        "org_id = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2696979c",
      "metadata": {
        "id": "2696979c"
      },
      "source": [
        "- **API Key**: es una clave de acceso que funciona como una contraseña para la autenticación con OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a4754d",
      "metadata": {
        "id": "f4a4754d",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##**Ejecute esta celda para ver el video.**\n",
        "from IPython.display import IFrame\n",
        "IFrame(\n",
        "        src=\"https://drive.google.com/file/d/15nroVfNSd6aETB6MYKveUnyyx6DbTIg8/preview\",\n",
        "        width=\"768px\",\n",
        "        height=\"432px\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f7e7720",
      "metadata": {
        "id": "7f7e7720"
      },
      "source": [
        "Debe agregar la clave en la siguiente variable:\n",
        "\n",
        "> **Nota**: debe tener mucho cuidado de **NO COMPARTIR** su API Key con ninguna persona. Con esta llave cualquier persona puede usar los servicios pagos de OpenAI y esto puede implicar pérdidas económicas. Evite compartir este notebook en ningún medio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca407569",
      "metadata": {
        "title": "[code]",
        "id": "ca407569"
      },
      "outputs": [],
      "source": [
        "api_key = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d38caf4",
      "metadata": {
        "id": "7d38caf4"
      },
      "source": [
        "Con estos dos elementos, podemos definir la autenticación con la librería `openai`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7585264",
      "metadata": {
        "title": "[code]",
        "id": "f7585264"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = api_key\n",
        "openai.organization = org_id"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a12e8396",
      "metadata": {
        "id": "a12e8396"
      },
      "source": [
        "## **2. Modelos**\n",
        "---\n",
        "\n",
        "OpenAI ofrece distintos LLMs basados en la arquitectura GPT, originalmente había dos variantes:\n",
        "\n",
        "1. **GPT (Generative Pre-trained Transformer)**: Es uno de los modelos más conocidos de OpenAI y ha pasado por varias iteraciones, incluyendo GPT-2 y GPT-3. Estos modelos se entrenaron en una gran cantidad de datos no supervisados y son capaces de generar texto coherente y relevante en diferentes contextos. Actualmente, es usado en herramientas como ChatGPT o BingChat.\n",
        "2. **Codex**: OpenAI también ha lanzado Codex, un modelo de lenguaje específicamente diseñado para programación. Codex puede comprender y generar código fuente en varios lenguajes de programación; ha demostrado ser una herramienta útil para desarrolladores y programadores. Actualmente, es usado en herramientas como Github Copilot o AWS CodeWhisperer.\n",
        "\n",
        "Estos modelos se utilizaban en tareas como autocompletado o edición de texto. No obstante, con la salida de GPT-3.5 (usado en ChatGPT) estos tipos de enfoques pasaron a estar desactualizados. Los modelos actuales cumplen ambas tareas y muchas otras más en forma de un chat inteligente, donde las tareas más específicas se pueden lograr por medio de instrucciones, lineamientos y guías. Más específicamente, estos modelos pueden cumplir tareas como:\n",
        "\n",
        "- Seguir instrucciones.\n",
        "- Question-answening.\n",
        "- Generación de texto.\n",
        "- Traducción automática.\n",
        "- Corrección de textos\n",
        "- Entre otras.\n",
        "\n",
        "Actualmente, en aplicaciones con texto se suelen usar principalmente dos modelos:\n",
        "\n",
        "- `GPT-3.5`: modelos que pueden entender y generar lenguaje natural y código.\n",
        "- `GPT-4`: modelos que tienen un mejor desempeño en comparación con GPT-3.5 y que pueden llegar a incluir otras modalidades de información como imágenes.\n",
        "\n",
        "Veamos los modelos disponibles desde _Python_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b8dff7",
      "metadata": {
        "title": "[code]",
        "id": "95b8dff7"
      },
      "outputs": [],
      "source": [
        "models = openai.Model.list()[\"data\"]\n",
        "names = list(map(lambda x: x[\"id\"], models))\n",
        "display(names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46b414a6",
      "metadata": {
        "id": "46b414a6"
      },
      "source": [
        "Como podemos ver, hay varios modelos. No obstante, la gran mayoría hacen parte de modelos de segunda y tercera generación que aún son soportados, pero no se recomienda su uso en aplicaciones nuevas. En este caso vamos a utilizar un modelo GPT-3.5 como el que se usa en ChatGPT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba6d862d",
      "metadata": {
        "title": "[code]",
        "id": "ba6d862d"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06a3ec81",
      "metadata": {
        "id": "06a3ec81"
      },
      "source": [
        "## **3. Chat**\n",
        "---\n",
        "\n",
        "Empezar a utilizar los LLMs de OpenAI desde _Python_ es un proceso sencillo y no requiere mucho entendimiento del funcionamiento de los modelos. Es decir, a diferencia de cuando trabajamos con la librería `transformers`, en este caso nunca llegaremos a interactuar directamente con un modelo, únicamente obtendremos la respuesta de la inferencia y el histórico de la conversación.\n",
        "\n",
        "Es necesario acotar los elementos generales de una conversación en OpenAI:\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1un8x9v1hLiH_2_ZUOilLEdGEmdWpI7NA\" width=\"100%\"> </center>\n",
        "\n",
        "- **Conversación**: secuencia de mensajes.\n",
        "- **Mensaje**: Texto asociado a un rol.\n",
        "- **Rol**: específica quién emitió el mensaje. Este puede ser `user` (la persona que interactúa con el chat), `assistant` (el modelo) o `system` (información del contexto).\n",
        "\n",
        "Adicionalmente, es importante entender que un **contexto** hace referencia a una temática general que se conserva a lo largo de toda la conversación. Normalmente, se usa para especializar el asistente con el que estamos conversando.\n",
        "\n",
        "Veamos un ejemplo de contexto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85f029e8",
      "metadata": {
        "title": "[code]",
        "id": "85f029e8"
      },
      "outputs": [],
      "source": [
        "context = \"Eres un programador experto en Python con conocimientos en procesamiento de lenguaje natural.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22c5330d",
      "metadata": {
        "id": "22c5330d"
      },
      "source": [
        "> **Nota**: el contexto muchas veces se utiliza para especificar la tarea que va a cumplir el modelo. Por esto, ya no es tan común seleccionar un tipo de modelo específico para una tarea puntual. En lugar usamos un modelo general y lo especializamos durante la conversación.\n",
        "\n",
        "Con esto, podemos comenzar a definir una conversación inicializada con el contexto. Cada mensaje debe ser un diccionario que contenga los campos `role` (Rol) y `content` (contenido del mensaje):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9ed73b1",
      "metadata": {
        "title": "[code]",
        "id": "b9ed73b1"
      },
      "outputs": [],
      "source": [
        "conversation = [{\"role\": \"system\", \"content\": context}]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c94c6891",
      "metadata": {
        "id": "c94c6891"
      },
      "source": [
        "Ahora, agregamos un mensaje con lo que deseamos consultar al modelo siguiendo la misma estructura de mensaje:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1887fc9",
      "metadata": {
        "title": "[code]",
        "id": "a1887fc9"
      },
      "outputs": [],
      "source": [
        "msg = {\"role\": \"user\", \"content\": \"Cómo puedo tokenizar un texto?\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3976acf3",
      "metadata": {
        "id": "3976acf3"
      },
      "source": [
        "Agregamos el mensaje a la conversación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e16f5473",
      "metadata": {
        "title": "[code]",
        "id": "e16f5473"
      },
      "outputs": [],
      "source": [
        "conversation.append(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74664dc0",
      "metadata": {
        "id": "74664dc0"
      },
      "source": [
        "Finalmente, utilizamos la utilidad de chat de `openai` para obtener una respuesta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1f29e7",
      "metadata": {
        "title": "[code]",
        "id": "6d1f29e7"
      },
      "outputs": [],
      "source": [
        "response = openai.ChatCompletion.create(model=model_name, messages=conversation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3865cf22",
      "metadata": {
        "id": "3865cf22"
      },
      "source": [
        "Veamos la respuesta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9a3cc5",
      "metadata": {
        "title": "[code]",
        "id": "5b9a3cc5"
      },
      "outputs": [],
      "source": [
        "display(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f792c9b4",
      "metadata": {
        "id": "f792c9b4"
      },
      "source": [
        "Vamos a agregar la respuesta a la conversación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0110c083",
      "metadata": {
        "title": "[code]",
        "id": "0110c083"
      },
      "outputs": [],
      "source": [
        "conversation.append(dict(response[\"choices\"][0][\"message\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ca4127b",
      "metadata": {
        "id": "8ca4127b"
      },
      "source": [
        "Podemos ver la conversación hasta este punto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def9be6c",
      "metadata": {
        "title": "[code]",
        "id": "def9be6c"
      },
      "outputs": [],
      "source": [
        "display(conversation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b179eaec",
      "metadata": {
        "id": "b179eaec"
      },
      "source": [
        "Vamos a agregar otro mensaje como respuesta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b661a9a2",
      "metadata": {
        "title": "[code]",
        "id": "b661a9a2"
      },
      "outputs": [],
      "source": [
        "msg = {\"role\": \"user\", \"content\": \"dame un ejemplo con spacy\"}\n",
        "conversation.append(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "317773cb",
      "metadata": {
        "id": "317773cb"
      },
      "source": [
        "Generamos la respuesta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5931adf2",
      "metadata": {
        "title": "[code]",
        "id": "5931adf2"
      },
      "outputs": [],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=conversation,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b752e44",
      "metadata": {
        "id": "3b752e44"
      },
      "source": [
        "Veamos la respuesta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7f85a27",
      "metadata": {
        "title": "[code]",
        "id": "a7f85a27"
      },
      "outputs": [],
      "source": [
        "display(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83011c50",
      "metadata": {
        "id": "83011c50"
      },
      "source": [
        "Por último, podemos utilizar hiperparámetros adicionales para modificar el comportamiento de los modelos con el método `create`. Entre estos tenemos:\n",
        "\n",
        "- `temperature`: Valor entre 0 y 2 que indica qué tan deterministas son las respuestas de los modelos. Entre más alto sea este valor, las respuestas pueden ser más creativas, pero también pueden ser incoherentes.\n",
        "- `top_p`: Es una alternativa al valor de temperatura. Se trata de una probabilidad acumulada (entre 0 y 1) para seleccionar los tokens más probables durante la generación.\n",
        "- `n`: Número de respuestas que se generarán.\n",
        "- `max_tokens`: Número máximo de tokens a generar en la respuesta.\n",
        "- `presence_penalty`: Número entre -2 y 2. Los valores positivos penalizan los nuevos tokens basado en si ya han sido usados anteriormente.\n",
        "- `frequency_penalty`: Número entre -2 y 2. Los valores positivos penalizan los nuevos tokens en dependencia de su ocurrencia en el texto.\n",
        "\n",
        "Veamos un ejemplo del efecto de la temperatura. Primero veamos un caso determinista con temperatura baja, la siguiente celda siempre generará la misma salida cuando la ejecute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b42e4229",
      "metadata": {
        "title": "[code]",
        "id": "b42e4229"
      },
      "outputs": [],
      "source": [
        "conv = [\n",
        "        {\"role\": \"system\", \"content\": \"Act as a linux terminal interpreter, I'll give you commands and you must generate the output as it the command was executed in a real terminal.\"},\n",
        "        {\"role\": \"user\", \"content\": \"ls\"}\n",
        "        ]\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=conv,\n",
        "    temperature=0.0\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d67958",
      "metadata": {
        "id": "34d67958"
      },
      "source": [
        "Veamos un ejemplo con un valor mayor de temperatura (si ejecuta la celda varias veces, el resultado puede variar) y acotando el número máximo de tokens a generar `max_tokens` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d72320",
      "metadata": {
        "title": "[code]",
        "id": "f0d72320"
      },
      "outputs": [],
      "source": [
        "conv = [\n",
        "        {\"role\": \"system\", \"content\": \"Act as a linux terminal interpreter, I'll give you commands and you must generate the output as it the command was executed in a real terminal.\"},\n",
        "        {\"role\": \"user\", \"content\": \"ls\"}\n",
        "        ]\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=model_name,\n",
        "    messages=conv,\n",
        "    temperature=2.0,\n",
        "    max_tokens=10\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e165ee4d",
      "metadata": {
        "id": "e165ee4d"
      },
      "source": [
        "## **Recursos Adicionales**\n",
        "---\n",
        "\n",
        "Los siguentes enlaces corresponden a las sitios donde encontrará información util para profundizar en los temas vistos en este taller guiado:\n",
        "\n",
        "- [OpenAI libraries](https://platform.openai.com/docs/libraries).\n",
        "- [OpenAI Models](https://platform.openai.com/docs/models)."
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "title,-all",
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}