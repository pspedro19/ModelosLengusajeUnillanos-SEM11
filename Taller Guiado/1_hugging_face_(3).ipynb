{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d90e7c20",
      "metadata": {
        "id": "d90e7c20"
      },
      "source": [
        "# Hugging Face\n",
        "---\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) es una empresa de tecnología con sede en Nueva York que se ha convertido en un líder en el campo del procesamiento del lenguaje natural (NLP - Natural Language Processing) y la inteligencia artificial (AI - Artificial Intelligence). Su principal contribución a la comunidad de AI es la creación de la biblioteca de software de código abierto llamada `transformers`. Esta biblioteca ofrece una amplia gama de herramientas y modelos de aprendizaje automático preentrenados que permiten a los desarrolladores y científicos de datos trabajar con NLP de manera eficiente y efectiva. Hugging Face también es conocida por su plataforma en línea, que proporciona acceso a una gran cantidad de modelos de lenguaje de vanguardia y permite a los usuarios compartir, explorar y colaborar en proyectos relacionados con NLP.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1ylT0PwS1zYIEi1LnlEBpvfyKfRhkat5V\" width=\"50%\"> </center>\n",
        "\n",
        "Hugging Face ofrece varias librerías para Python que facilitan el desarrollo y la implementación de grandes modelos del lenguaje (LLMs - Large Language Models). Algunas de las principales librerías proporcionadas por Hugging Face son:\n",
        "\n",
        "1. `transformers`: Esta es la librería central de Hugging Face que permite trabajar con LLMs preentrenados para realizar inferencia en tareas como la generación de texto, clasificación de texto, traducción, entre otras. Proporciona una interfaz fácil de usar para cargar, entrenar y utilizar modelos basados en transformers, como BERT, GPT, RoBERTa, entre otros.\n",
        "2. `tokenizers`: Esta librería se centra en el tokenizado de texto (separación de cadenas de caracteres en unidades lógicas como caracteres, palabras u oraciones) que es un paso crucial en el procesamiento del lenguaje natural. Tokenizers ofrece una variedad de algoritmos de tokenización eficientes y personalizables según el modelo utilizado.\n",
        "3. `datasets`: Esta librería proporciona un conjunto de herramientas para trabajar con conjuntos de datos de NLP. Ofrece una amplia colección de conjuntos de datos de texto comunes, así como utilidades para cargar, procesar y dividir los datos en entrenamiento, validación y prueba.\n",
        "4. `accelerate`: Esta librería ayuda a acelerar el entrenamiento y la inferencia de LLMs utilizando técnicas de paralelización y distribución. Permite aprovechar el poder de múltiples GPUs o TPUs para acelerar el procesamiento de datos.\n",
        "\n",
        "Además de estas librerías principales, Hugging Face también proporciona otras herramientas y recursos relacionados, como pipelines para tareas específicas de procesamiento del lenguaje natural, una plataforma de modelado llamada \"Model Hub\" para compartir y descubrir modelos entrenados, y una comunidad activa en línea para obtener soporte y compartir conocimientos.\n",
        "\n",
        "Es importante entender que estas librerías operan a distintos niveles dentro de un pipeline clásico de NLP, tal y como se muestra en la siguiente figura:\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1Z5Wr3mrrDjtsY2KxQ-4Z8zoO6E72yxnj\" width=\"80%\"> </center>\n",
        "\n",
        "Comenzaremos cargando las librerías base:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4667e0b9",
      "metadata": {
        "title": "[code]",
        "id": "4667e0b9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaffe8b6",
      "metadata": {
        "id": "eaffe8b6"
      },
      "source": [
        "## **1. Datasets**\n",
        "---\n",
        "\n",
        "La librería `datasets` de Hugging Face permite trabajar con diversos conjuntos de datos, incluyendo textos para NLP, imágenes para visión por computador, audios, entre otros. Adicional a esto, la librería proporciona una amplia gama de conjuntos de datos populares y listos para usar. Con `datasets`, los desarrolladores y científicos de datos pueden cargar fácilmente conjuntos de datos, explorar su contenido y realizar operaciones de preprocesamiento y transformación. La librería ofrece una interfaz consistente y fácil de usar para acceder a los datos, lo que facilita la creación de pipelines de procesamiento de datos eficientes y reproducibles.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1VNQAC3KICyjFOwPytkpyhgmWn46wrdKt\" width=\"80%\"> </center>\n",
        "\n",
        "Además, `datasets` permite dividir automáticamente los datos en conjuntos de entrenamiento, validación y prueba, y proporciona utilidades para la evaluación y métricas de rendimiento. Con su enfoque en la calidad, la accesibilidad y la versatilidad, la librería \"datasets\" de Hugging Face se ha convertido en una opción popular para la gestión de conjuntos de datos en el campo de NLP y asimismo para trabajar con LLMs.\n",
        "\n",
        "Comenzaremos con la instalación de `datasets`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff5cf31b",
      "metadata": {
        "title": "[code]",
        "id": "ff5cf31b"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f21492a",
      "metadata": {
        "id": "7f21492a"
      },
      "source": [
        "Veamos un ejemplo con el conjunto de datos `rotten_tomatoes`. Este conjunto de datos se construye a partir de críticas de películas recopiladas del sitio web [Rotten Tomatoes](https://www.rottentomatoes.com/), que es conocido por su agregación de reseñas de películas y puntuaciones de críticos. El conjunto de datos `rottent_tomatoes` contiene una gran cantidad de comentarios de películas, junto con su correspondiente etiqueta de sentimiento, que indica si la crítica es positiva o negativa. Está etiquetado en un esquema binario, donde 1 representa una crítica positiva y 0 representa una crítica negativa.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1XonJg9EHPA0wqvLzO00T26kVRPEVNpHK\" width=\"80%\"> </center>\n",
        "\n",
        "Este conjunto de datos es valioso para tareas de análisis de sentimientos, clasificación de texto y modelado del lenguaje, ya que proporciona una amplia variedad de opiniones sobre películas y permite entrenar modelos para comprender y clasificar la polaridad de las críticas de manera efectiva. Con `rottent_tomatoes`, los investigadores y desarrolladores pueden acceder a un conjunto de datos confiable y diverso para abordar una amplia gama de problemas relacionados con el procesamiento del lenguaje natural en el ámbito del cine y las críticas de películas. Una descripción detallada del conjunto de datos y su formato se encuentra en [Hugging Face](https://huggingface.co/datasets/rotten_tomatoes).\n",
        "\n",
        "Para cargar el conjunto de datos, utilizaremos la función `load_dataset` la cual nos permite descargar y cargar conjuntos de datos siguiendo la estructura de `datasets`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0035ab8a",
      "metadata": {
        "title": "[code]",
        "id": "0035ab8a"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c5e25fb",
      "metadata": {
        "id": "3c5e25fb"
      },
      "source": [
        "Ahora, cargamos el conjunto de datos especificando la partición de datos a utilizar con el parámetro `split`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "444edf11",
      "metadata": {
        "title": "[code]",
        "id": "444edf11"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
        "display(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af6d831",
      "metadata": {
        "id": "1af6d831"
      },
      "source": [
        "Podemos ver el número de muestras accediendo al atributo `num_rows`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ac7bc3",
      "metadata": {
        "title": "[code]",
        "id": "86ac7bc3"
      },
      "outputs": [],
      "source": [
        "display(ds.num_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b269b674",
      "metadata": {
        "id": "b269b674"
      },
      "source": [
        "De la misma forma, podemos extraer muestras de este dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52f02110",
      "metadata": {
        "title": "[code]",
        "id": "52f02110"
      },
      "outputs": [],
      "source": [
        "display(ds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee0e6f66",
      "metadata": {
        "id": "ee0e6f66"
      },
      "source": [
        "> **Nota**: La librería `datasets` ofrece varios mecanismos para carga y transformación de textos. Lo veremos en detalle en la unidad 2."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9229b18f",
      "metadata": {
        "id": "9229b18f"
      },
      "source": [
        "## **2. Tokenizers**\n",
        "---\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1MyOXWqZxeR8lfunx-UePSUtJdHeJ5amn\" width=\"80%\"> </center>\n",
        "\n",
        "La librería `tokenizers` de Hugging Face es una herramienta versátil y eficiente para el tokenizado de texto. El tokenizado es el proceso de dividir el texto en unidades más pequeñas, como palabras o subpalabras, que son más fáciles de procesar por los LLMs. La librería `tokenizers` ofrece una amplia gama de algoritmos de tokenización que se adaptan a diversas necesidades y requisitos lingüísticos.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1hV0JdeXEYOwu1cuZBZ4Cfb__zowTHzCt\" width=\"50%\"> </center>\n",
        "\n",
        "Esta librería permite a los desarrolladores personalizar y configurar el proceso de tokenización de acuerdo con sus necesidades específicas. Se pueden aplicar diferentes estrategias de tokenización, como tokenización basada en reglas, tokenización basada en espacios en blanco, tokenización basada en subpalabras (como BPE o Byte Pair Encoding) y tokenización basada en caracteres. Además, `tokenizers` proporciona una interfaz fácil de usar para entrenar nuevos tokenizadores a partir de corpus de texto personalizados, lo que permite adaptar el proceso de tokenización a dominios específicos.\n",
        "\n",
        "Una de las ventajas de la librería 'tokenizers' es su eficiencia y capacidad de procesamiento rápido. Está implementada en C++ y se integra perfectamente con Python, lo que permite un rendimiento óptimo incluso al procesar grandes volúmenes de texto. `tokenizers` también es compatible con diferentes formatos de entrada y salida, lo que facilita la integración con otras librerías y flujos de trabajo de procesamiento del lenguaje natural. Además, ofrece una interfaz sencilla para acceder a los tokens resultantes, así como para obtener información adicional, como los desplazamientos de caracteres correspondientes a cada token.\n",
        "\n",
        "Para algunos modelos pueden haber algunas dependencias adicionales, en este caso es requerida la librería `sentencepiece` y `sacremoses` para obtener tokenizadores adicionales:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c081259c",
      "metadata": {
        "title": "[code]",
        "id": "c081259c"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers sentencepiece sacremoses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f1d0cf",
      "metadata": {
        "id": "e7f1d0cf"
      },
      "source": [
        "Con `tokenizers` podemos crear flujos de preprocesamiento de texto personalizados por medio de herramientas internas y librerías externas. De igual forma, podemos utilizar algunos tokenizadores predefinidos que son ampliamente usados en los LLMs más populares de hoy en día.\n",
        "\n",
        "Por ejemplo, BERT (Bidirectional Encoder Representations from Transformers) es un modelo de lenguaje basado en transformers que ha tenido un gran impacto en el procesamiento del lenguaje natural (NLP). Desarrollado por Google en 2018, BERT es conocido por su capacidad para capturar el contexto y las relaciones semánticas en el texto de una manera profunda y bidireccional.\n",
        "\n",
        "Muchos de los elementos de este modelo se han reutilizado en otras arquitecturas. Por esto mismo, podemos encontrar un tokenizador encargado del preprocesamiento necesario para transformar un texto en una entrada compatible con el modelo.\n",
        "\n",
        "Veamos cómo importar este tokenizador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5294bce3",
      "metadata": {
        "title": "[code]",
        "id": "5294bce3"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbf25b10",
      "metadata": {
        "id": "fbf25b10"
      },
      "source": [
        "Para cargar un tokenizador, utilizamos el método `from_pretrained`, este nos permite cargar un tokenizer pre-entrenado desde el Hub de Huggingface a partir del [nombre](https://huggingface.co/bert-base-uncased) que este tenga registrado, en este caso `bert-base-uncased`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23bee9ac",
      "metadata": {
        "title": "[code]",
        "id": "23bee9ac"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "display(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8996432a",
      "metadata": {
        "id": "8996432a"
      },
      "source": [
        "> **Nota**: la creación de un tokenizador normalmente involucra el uso de un conjunto de datos. `bert-base-uncased` es un tokenizador que ya fue entrenado sobre un corpus en inglés. En la unidad 2 veremos cómo podemos definir tokenizadores más específicos.\n",
        "\n",
        "El tokenizador tiene algunos métodos que nos permiten manipular el texto. Por ejemplo, el método `encode` permite tokenizar un texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a38bee1",
      "metadata": {
        "title": "[code]",
        "id": "3a38bee1"
      },
      "outputs": [],
      "source": [
        "output = tokenizer.encode(\"hello, this is a basic example.\")\n",
        "display(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaaec0fb",
      "metadata": {
        "id": "aaaec0fb"
      },
      "source": [
        "Podemos extraer los tokens con el atributo `tokens`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a26d854",
      "metadata": {
        "title": "[code]",
        "id": "5a26d854"
      },
      "outputs": [],
      "source": [
        "display(output.tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9ab4be6",
      "metadata": {
        "id": "e9ab4be6"
      },
      "source": [
        "El atributo `offsets` nos permite extraer las posiciones en las que ocurre cada token dentro del texto original:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d57c0c15",
      "metadata": {
        "title": "[code]",
        "id": "d57c0c15"
      },
      "outputs": [],
      "source": [
        "display(output.offsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68e0d52a",
      "metadata": {
        "id": "68e0d52a"
      },
      "source": [
        "Finalmente, la representación numérica la podemos extraer con el atributo `ids`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d8b39ed",
      "metadata": {
        "title": "[code]",
        "id": "1d8b39ed"
      },
      "outputs": [],
      "source": [
        "display(output.ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c449cb22",
      "metadata": {
        "id": "c449cb22"
      },
      "source": [
        "Podemos saber a qué número corresponde cada palabra con el método `token_to_id`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d95e6250",
      "metadata": {
        "title": "[code]",
        "id": "d95e6250"
      },
      "outputs": [],
      "source": [
        "display(tokenizer.token_to_id(\"hello\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0805ced8",
      "metadata": {
        "id": "0805ced8"
      },
      "source": [
        "De la misma forma, podemos obtener a qué token corresponde un número o secuencia de números con el método `decode`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be12be43",
      "metadata": {
        "title": "[code]",
        "id": "be12be43"
      },
      "outputs": [],
      "source": [
        "display(tokenizer.decode([7592]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "261fc1fa",
      "metadata": {
        "id": "261fc1fa"
      },
      "source": [
        "## **3. Modelos**\n",
        "---\n",
        "\n",
        "Los LLMs son redes neuronales profundas basadas en la arquitectura de transformer. Existen distintos modelos y variaciones de un mismo modelo; por ejemplo, el modelo BERT se entrenó originalmente con un corpus en Inglés, posteriormente, el mismo modelo fue entrenado en otros idiomas e incluso de forma multi-lenguaje.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1TnUXDDAHUS_8N5G5UJlIZju7eMKKbODi\" width=\"80%\"> </center>\n",
        "\n",
        "La librería `transformers` nos permite utilizar las arquitecturas de LLMs populares, entrenar modelos y reutilizar modelos que ya han sido entrenados por otras personas de la comunidad. Todo esto sobre las librerías más típicas para deep learning como `pytorch`, `tensorflow` o `jax`.\n",
        "\n",
        "Veamos cómo instalar la librería:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "973c4866",
      "metadata": {
        "title": "[code]",
        "id": "973c4866"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5c70928",
      "metadata": {
        "id": "e5c70928"
      },
      "source": [
        "> **Nota**: es posible utilizar `transformers` con `tensorflow` instalando `transformers[tf-gpu]` o `transformers[tf-cpu]`, también con `jax` como `transformers[jax]`. No obstante, la mayor cantidad de modelos pre-entrenados se encuentran en `torch` y es la librería que mayor soporte tiene.\n",
        "\n",
        "En `transformers` podemos encontrar los siguientes LLM para tareas de NLP:\n",
        "\n",
        "- `ForMaskedLM`: son modelos de llenado de máscaras (interpolación de palabras).\n",
        "- `ForMultipleChoice`: son modelos usados para toma de decisiones, esto difiere de un problema de clasificación clásico donde únicamente hay una categoría correcta. En este caso, pueden haber varias respuestas correctas o alguna más correcta que otra.\n",
        "- `ForNextSentencePrediction`: son modelos para identificar si una oración occure antes o después de un fragmento de texto.\n",
        "- `ForQuestionAnswering`: son modelos que permiten dar respuesta a una pregunta dado un texto o contexto base.\n",
        "- `BertForSequenceClassification`: son modelos pensados en clasificación general de secuencias de texto.\n",
        "- `ForTokenClassification`: son modelos para clasificación de textos, muy comunes en tareas como part-of-speech o named entity recognition.\n",
        "- `Generation`: son modelos pensados en la generación automática de textos.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1OsqTyeu4TzWmUGXBxgbHxmJSfmqmE6TP\" width=\"90%\"> </center>\n",
        "\n",
        "Veamos cómo podemos cargar una versión del modelo Bert para clasificación por medio de la clase `BertForSequenceClassification`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f108d7",
      "metadata": {
        "title": "[code]",
        "id": "35f108d7"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "516fecc5",
      "metadata": {
        "id": "516fecc5"
      },
      "source": [
        "Podemos descargar y cargar un modelo Bert entrenado para análisis de sentimientos como `finiteautomata/bertweet-base-sentiment-analysis`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5141b402",
      "metadata": {
        "title": "[code]",
        "id": "5141b402"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
        ")\n",
        "display(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b39e3e69",
      "metadata": {
        "id": "b39e3e69"
      },
      "source": [
        "Se trata de un modelo que fue entrenado sobre el conjunto de datos **SemEval** del 2017 (con alrededor de 40 mil tweets) que permite clasificar textos en las categorías POS (positivo), NEG (negativo) y NEU (neutral).\n",
        "\n",
        "> **Nota**: los modelos cargados se encuentran en `torch`, en la siguiente unidad veremos cómo podemos cómo podemos manipularlos desde esta librería.\n",
        "\n",
        "En este punto, podemos utilizar el `dataset` junto con el `tokenizer` y el `transformer` para generar una predicción. Primero vamos a extraer un texto del conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f86f339",
      "metadata": {
        "title": "[code]",
        "id": "1f86f339"
      },
      "outputs": [],
      "source": [
        "text = ds[0][\"text\"]\n",
        "label = ds[0][\"label\"]\n",
        "display(text)\n",
        "display(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae01e113",
      "metadata": {
        "id": "ae01e113"
      },
      "source": [
        "Ahora, vamos a cargar el tokenizador que fue definido para este modelo por medio de la clase `BertweetTokenizer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b15af9",
      "metadata": {
        "title": "[code]",
        "id": "c5b15af9"
      },
      "outputs": [],
      "source": [
        "from transformers import BertweetTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b30576e",
      "metadata": {
        "id": "6b30576e"
      },
      "source": [
        "Cargamos el tokenizador de este modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5254ac0d",
      "metadata": {
        "title": "[code]",
        "id": "5254ac0d"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertweetTokenizer.from_pretrained(\n",
        "    \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
        ")\n",
        "display(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acbdf180",
      "metadata": {
        "id": "acbdf180"
      },
      "source": [
        "Ahora, extraemos los vectores en `torch` que funcionarán como entrada del modelo, esto es posible con el parámetro `return_tensors` indicando las siglas de `pt` de `pytorch`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab959bc8",
      "metadata": {
        "title": "[code]",
        "id": "ab959bc8"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "display(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8d06117",
      "metadata": {
        "id": "a8d06117"
      },
      "source": [
        "Finalmente, le pasamos la entrada al modelo para generar una predicción:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "604f650f",
      "metadata": {
        "title": "[code]",
        "id": "604f650f"
      },
      "outputs": [],
      "source": [
        "pred = model(**inputs)\n",
        "display(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce1f9e9e",
      "metadata": {
        "id": "ce1f9e9e"
      },
      "source": [
        "> **Nota**: recuerde que en _Python_ el operador `**` permite des-estructurar diccionarios dentro de funciones. De tal forma que las llaves actúan como parámetros y los valores como argumentos.\n",
        "\n",
        "En este caso el modelo retorna una predicción que corresponde a los *logits* de la clasificación en las tres posibles clases. Es importante recordar que entre más positivo sea el valor de un logit, más probable es la clase correspondiente. Podemos obtener la clase predicha con el método `argmax`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5418e627",
      "metadata": {
        "title": "[code]",
        "id": "5418e627"
      },
      "outputs": [],
      "source": [
        "y_pred = pred.logits.argmax()\n",
        "display(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68fe3dc1",
      "metadata": {
        "id": "68fe3dc1"
      },
      "source": [
        "Por último, para saber a qué categoría corresponde el número predicho. Podemos acceder al diccionario de conversión disponible dentro de la configuración del modelo en el atributo `config.id2label`. Veamos el resultado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d9cc3cf",
      "metadata": {
        "title": "[code]",
        "id": "6d9cc3cf"
      },
      "outputs": [],
      "source": [
        "display(model.config.id2label[int(y_pred)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bb4ae26",
      "metadata": {
        "id": "5bb4ae26"
      },
      "source": [
        "## **4. Inferencia**\n",
        "---\n",
        "\n",
        "Una de las ventajas de la librería `transformers` es que nos permite realizar inferencia (obtener predicciones) a partir de modelos pre-entrenados sin la necesidad de repetir manualmente todos los pasos anteriores.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1m5qO9R-g-0jW6TFjq0Wd1eomdNgVNKad\" width=\"80%\"> </center>\n",
        "\n",
        "Para esto, `transformers` nos permite utilizar los `pipelines`, que permite encapsular los tokenizadores y los modelos con el fin de tener un objeto que genere predicciones únicamente a partir del texto.\n",
        "\n",
        "Veamos un ejemplo con el modelo que utilizamos como ejemplo anteriormente. Primero, cargamos la utilidad de `pipeline`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8174364d",
      "metadata": {
        "title": "[code]",
        "id": "8174364d"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe9b2112",
      "metadata": {
        "id": "fe9b2112"
      },
      "source": [
        "Ahora, cargamos el `pipeline` correspondiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "098552d3",
      "metadata": {
        "title": "[code]",
        "id": "098552d3"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    model=\"finiteautomata/bertweet-base-sentiment-analysis\", task=\"text-classification\"\n",
        ")\n",
        "display(pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "206e23f6",
      "metadata": {
        "id": "206e23f6"
      },
      "source": [
        "El `pipeline` puede ser utilizado como cualquier función a partir del texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92bb1345",
      "metadata": {
        "title": "[code]",
        "id": "92bb1345"
      },
      "outputs": [],
      "source": [
        "pred = pipe(\"I'm so happy today, it's my birthday.\")\n",
        "display(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28e9ff54",
      "metadata": {
        "id": "28e9ff54"
      },
      "source": [
        "Como podemos ver, el resultado ya contiene la etiqueta predicha y la probabilidad de que pertenezca a esta categoría.\n",
        "\n",
        "Es importante saber que un `pipeline` nos ofrece una forma transparente de utilizar los LLMs en distintas tareas sin la necesidad de preocuparnos por los detalles de más bajo nivel. Veamos algunos detalles y tareas importantes que involucran NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c6248cd",
      "metadata": {
        "id": "2c6248cd"
      },
      "source": [
        "### **4.1. Búsqueda de Modelos**\n",
        "---\n",
        "\n",
        "Uno de los elementos clave para realizar inferencia con Hugging Face, es saber navegar y encontrar modelos para una tarea específica por medio del [Hub](https://huggingface.co/models).\n",
        "\n",
        "Como podemos ver en el siguiente video, es posible filtrar por tipo de dato, tipo de tarea, idioma, y encontrar un tipo de modelo específico a nuestras necesidades:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "567e3a3d",
      "metadata": {
        "title": "[code]",
        "id": "567e3a3d",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown ##**Ejecute esta celda para ver el video.**\n",
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(\n",
        "    src=\"https://drive.google.com/file/d/1epWBAQ8G4n4lIRmUTznRbJRpVQUEucLt/preview\",\n",
        "    width=\"768px\",\n",
        "    height=\"432px\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb87c1a3",
      "metadata": {
        "id": "cb87c1a3"
      },
      "source": [
        "Veamos las tareas que podemos solucionar con `transformers`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6521222",
      "metadata": {
        "id": "e6521222"
      },
      "source": [
        "### **4.2. Clasificación de Tokens**\n",
        "---\n",
        "\n",
        "La clasificación de tokens con LLMs (Language Model for Classification of Tokens) se refiere al proceso de asignar etiquetas o categorías a cada token en un texto utilizando modelos de lenguaje. Estos modelos se entrenan para aprender las relaciones semánticas y las características asociadas con cada token, lo que les permite comprender y predecir las etiquetas de clasificación. Al pasar nuevos tokens por el LLM, se obtiene una predicción de la etiqueta o categoría asociada a cada uno. La clasificación de tokens con LLMs es útil en diversas aplicaciones de procesamiento del lenguaje natural, como la detección de entidades, el etiquetado gramatical o la identificación de emociones, ya que aprovecha el conocimiento lingüístico aprendido durante el entrenamiento para realizar clasificaciones precisas y automatizadas.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1JxkUKO13ndsAMYPuMhxY-HnRCWRSU2Db\" width=\"60%\"> </center>\n",
        "\n",
        "Veamos un ejemplo con el modelo `robertuilo` para part-of-speech (función gramátical de cada palabra en el texto) en español. Cargamos el `pipeline` con la tarea `token-classification`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b57693e9",
      "metadata": {
        "title": "[code]",
        "id": "b57693e9"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    model=\"pysentimiento/robertuito-pos\",\n",
        "    task=\"token-classification\",\n",
        ")\n",
        "display(pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0ae4417",
      "metadata": {
        "id": "c0ae4417"
      },
      "source": [
        "Ahora, realizamos la inferencia:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "151721fd",
      "metadata": {
        "title": "[code]",
        "id": "151721fd"
      },
      "outputs": [],
      "source": [
        "res = pipe(\"Estamos aprendiendo a usar los grandes modelos del lenguaje\")\n",
        "display(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09e45838",
      "metadata": {
        "id": "09e45838"
      },
      "source": [
        "Como podemos ver, la salida muestra cada token y su respectiva categorización como `entity`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "775d48f7",
      "metadata": {
        "id": "775d48f7"
      },
      "source": [
        "### **4.3. Clasificación de Secuencias**\n",
        "---\n",
        "\n",
        "La clasificación de secuencias con LLMs (Language Model for Classification of Sequences) se refiere al proceso de asignar etiquetas o categorías a secuencias de texto utilizando modelos de lenguaje. Estos modelos se entrenan para comprender el contexto y las relaciones entre las palabras en una secuencia y, a partir de ese conocimiento, realizar predicciones de clasificación. La clasificación de secuencias con LLMs puede abordar diversas tareas, como la clasificación de sentimientos en textos, la detección de intenciones en oraciones o la categorización de documentos. Al pasar una secuencia por el LLM se obtiene una predicción de la etiqueta o categoría correspondiente a esa secuencia en función de su contexto y contenido. La clasificación de secuencias con LLMs es un enfoque poderoso en el procesamiento del lenguaje natural, ya que los modelos de lenguaje pueden capturar características y patrones complejos en las secuencias, lo que permite una clasificación precisa y automática.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1CJmYtbOd7aPlQWFJOS03Aky6LDV8wDvQ\" width=\"100%\"> </center>\n",
        "\n",
        "Veamos un ejemplo con el modelo `roberta` para la detección automática de idiomas. Cargamos el `pipeline` para la tarea de `text-classification`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2405440",
      "metadata": {
        "title": "[code]",
        "id": "c2405440"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    model=\"papluca/xlm-roberta-base-language-detection\", task=\"text-classification\"\n",
        ")\n",
        "display(pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7cea0cb",
      "metadata": {
        "id": "e7cea0cb"
      },
      "source": [
        "Veamos cómo funciona la detección de idiomas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50515318",
      "metadata": {
        "title": "[code]",
        "id": "50515318"
      },
      "outputs": [],
      "source": [
        "res = pipe(\"Hello, how are you today?\")\n",
        "display(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d35135",
      "metadata": {
        "title": "[code]",
        "id": "88d35135"
      },
      "outputs": [],
      "source": [
        "res = pipe(\"Hola, cómo te encuentras?\")\n",
        "display(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfa02868",
      "metadata": {
        "id": "dfa02868"
      },
      "source": [
        "### **4.4. Traducción Neuronal**\n",
        "---\n",
        "\n",
        "La traducción neuronal con LLMs (Language Model for Neural Machine Translation) se refiere al enfoque de utilizar modelos de lenguaje basados en redes neuronales para realizar la traducción automática de texto entre diferentes idiomas. Estos modelos se entrenan para aprender a generar traducciones precisas y coherentes utilizando técnicas de aprendizaje profundo y arquitecturas de redes neuronales. Durante el entrenamiento, los LLMs se exponen a pares de oraciones en diferentes idiomas, donde la oración de origen se presenta como entrada y la oración objetivo como salida deseada. El modelo aprende a capturar las relaciones lingüísticas y las correspondencias entre las palabras en ambos idiomas, lo que le permite generar traducciones de calidad. La traducción neuronal con LLMs ha demostrado ser efectiva en mejorar la precisión y la fluidez de las traducciones automáticas, superando en muchos casos a los enfoques tradicionales basados en reglas. Este enfoque ha revolucionado el campo de la traducción automática al permitir una mayor comprensión del contexto y las sutilezas lingüísticas, brindando una solución más cercana a la traducción humana."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01997c7b",
      "metadata": {
        "title": "[code]",
        "id": "01997c7b",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown ##**Ejecute esta celda para ver el video.**\n",
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(\n",
        "    src=\"https://drive.google.com/file/d/1tVfPl6FT56ONYLRIvaCVOUAx0IYNHcyB/preview\",\n",
        "    width=\"768px\",\n",
        "    height=\"432px\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d25326",
      "metadata": {
        "id": "d4d25326"
      },
      "source": [
        "Veamos un ejemplo con un transformer general para traducción de español a inglés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69148414",
      "metadata": {
        "title": "[code]",
        "id": "69148414"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    model=\"Helsinki-NLP/opus-mt-es-en\",\n",
        "    task=\"translation\",\n",
        ")\n",
        "display(pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d139ec6b",
      "metadata": {
        "id": "d139ec6b"
      },
      "source": [
        "Veamos un ejemplo de traducción:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8321904f",
      "metadata": {
        "title": "[code]",
        "id": "8321904f"
      },
      "outputs": [],
      "source": [
        "res = pipe(\"Hola, está todo bien?\")\n",
        "display(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8a882c",
      "metadata": {
        "id": "6b8a882c"
      },
      "source": [
        "### **4.5. Llenado de Máscaras**\n",
        "---\n",
        "\n",
        "El llenado de máscaras, también conocido como Masked Language Modeling en inglés, es una técnica utilizada en modelos de lenguaje para entrenar y evaluar su capacidad de comprensión del contexto y su habilidad para inferir palabras o tokens faltantes en una secuencia de texto. Durante el entrenamiento, se seleccionan aleatoriamente algunos tokens de la secuencia y se los \"mascara\" o enmascaran, es decir, se los oculta temporalmente. El modelo debe predecir los tokens enmascarados basándose en el contexto circundante y el conocimiento adquirido durante el entrenamiento. Esta técnica ayuda al modelo a aprender representaciones más profundas y significativas de las palabras, ya que debe capturar las relaciones semánticas y gramaticales para realizar predicciones precisas. Además, el llenado de máscaras es utilizado también para evaluar la calidad del modelo al presentarle secuencias con tokens enmascarados y comparar sus predicciones con los tokens reales. Esta técnica ha sido ampliamente aplicada en modelos de lenguaje como BERT y ha demostrado ser efectiva para mejorar el rendimiento en diversas tareas de procesamiento del lenguaje natural, como la clasificación de texto, la extracción de información y la generación de texto.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1dFeTvEdOjN0Uo9JfvYCkvdUpgdqX2CNd\" width=\"80%\"> </center>\n",
        "\n",
        "Veamos un ejemplo de llenado de máscaras con el modelo `roberta`. Comenzamos cargando el `pipeline` con la tarea de `fill-mask`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2ba9ce8",
      "metadata": {
        "title": "[code]",
        "id": "d2ba9ce8"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    model=\"xlm-roberta-base\",\n",
        "    task=\"fill-mask\",\n",
        ")\n",
        "display(pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e96562b8",
      "metadata": {
        "id": "e96562b8"
      },
      "source": [
        "En este caso, la palabra que será predicha estará dada por el token `<mask>`. Veamos el ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47be555",
      "metadata": {
        "title": "[code]",
        "id": "f47be555"
      },
      "outputs": [],
      "source": [
        "res = pipe(\"Pedro compró un nuevo <mask>\")\n",
        "display(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00839050",
      "metadata": {
        "id": "00839050"
      },
      "source": [
        "### **4.6. Resumen Abstractivo**\n",
        "---\n",
        "\n",
        "El resumen abstractivo de texto con LLMs (Language Model for Abstractive Text Summarization) se refiere a la técnica de utilizar modelos de lenguaje para generar resúmenes concisos y coherentes de textos largos o documentos extensos. A diferencia del enfoque extractivo, donde se seleccionan y combinan fragmentos del texto original, el resumen abstractivo busca comprender el contenido y el contexto del texto para generar resúmenes que incluyan información nueva y relevantes. Los LLMs se entrenan en grandes conjuntos de datos de texto con pares de texto original y resumen objetivo, y aprenden a capturar las relaciones y la estructura del lenguaje para producir resúmenes de alta calidad. Durante la generación del resumen, el modelo utiliza su conocimiento previo para identificar las ideas principales y expresarlas de manera coherente y comprensible. El resumen abstractivo con LLMs es una técnica prometedora en el procesamiento del lenguaje natural, ya que permite automatizar la creación de resúmenes que resalten la información clave de manera más natural y humana.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1YmiFZJGF5FQMQZ9phk1e6Ecq8aDMP20I\" width=\"100%\"> </center>\n",
        "\n",
        "Veamos un ejemplo con el modelo `mt5` para resumen en español. Comenzamos cargando el `pipeline` para la tarea `summarization`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aada7e42",
      "metadata": {
        "title": "[code]",
        "id": "aada7e42"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    model=\"josmunpen/mt5-small-spanish-summarization\",\n",
        "    task=\"summarization\",\n",
        ")\n",
        "display(pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "089162b2",
      "metadata": {
        "id": "089162b2"
      },
      "source": [
        "Veamos un ejemplo de resumen sobre un fragmento de la biografía de Allan Turing:\n",
        "\n",
        ">Durante la segunda guerra mundial, trabajó en descifrar los códigos nazis, particularmente los de la máquina Enigma, y durante un tiempo fue el director de la sección Naval Enigma de Bletchley Park. Se ha estimado que su trabajo acortó la duración de esa guerra entre dos y cuatro años. Tras la guerra, diseñó uno de los primeros computadores electrónicos programables digitales en el Laboratorio Nacional de Física del Reino Unido y poco tiempo después construyó otra de las primeras máquinas en la Universidad de Mánchester."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "465c7cb6",
      "metadata": {
        "title": "[code]",
        "id": "465c7cb6"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Durante la segunda guerra mundial, trabajó en descifrar los códigos nazis, particularmente los de la máquina Enigma, y durante un tiempo fue el director de la sección Naval Enigma de Bletchley Park. Se ha estimado que su trabajo acortó la duración de esa guerra entre dos y cuatro años. Tras la guerra, diseñó uno de los primeros computadores electrónicos programables digitales en el Laboratorio Nacional de Física del Reino Unido y poco tiempo después construyó otra de las primeras máquinas en la Universidad de Mánchester.\n",
        "\"\"\"\n",
        "pprint(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fcb4618",
      "metadata": {
        "id": "2fcb4618"
      },
      "source": [
        "Ahora aplicamos el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c979d33",
      "metadata": {
        "title": "[code]",
        "id": "5c979d33"
      },
      "outputs": [],
      "source": [
        "res = pipe(text)\n",
        "display(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd87b8f",
      "metadata": {
        "id": "8fd87b8f"
      },
      "source": [
        "### **4.7. Question Answering**\n",
        "---\n",
        "\n",
        "Question Answering (QA) con LLMs (Language Model for Question Answering) se refiere al enfoque de utilizar modelos de lenguaje basados en redes neuronales para responder preguntas de manera automática. Estos modelos se entrenan en conjuntos de datos que contienen pares de preguntas y respuestas correspondientes. Durante el entrenamiento, el LLM aprende a capturar la relación entre la pregunta y la respuesta, así como el contexto necesario para proporcionar respuestas precisas. Cuando se le presenta una pregunta, el modelo genera una respuesta basada en su conocimiento previo y en el análisis del texto de referencia o contexto relacionado. El question answering con LLMs ha demostrado ser efectivo en una amplia gama de aplicaciones, desde la búsqueda de información hasta la asistencia virtual. Esta técnica ha permitido avances significativos en la capacidad de las máquinas para comprender preguntas complejas y proporcionar respuestas relevantes y comprensibles, mejorando así la interacción entre humanos y sistemas de procesamiento del lenguaje natural.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1RcLJ8nG3OXkO01Kn4TLL2iY9eBo1oJQ3\" width=\"100%\"> </center>\n",
        "\n",
        "En QA el modelo parte de dos elementos:\n",
        "\n",
        "- `contexto`: texto original de dónde se dará la respuesta.\n",
        "- `pregunta`: pregunta a responder.\n",
        "\n",
        "Con esto, el modelo puede generar una respuesta. Veamos un ejemplo con el modelo `roberta` para QA en español. Comenzamos cargando el modelo con la tarea `question-answering`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2501edd",
      "metadata": {
        "title": "[code]",
        "id": "c2501edd"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(model=\"PlanTL-GOB-ES/roberta-base-bne-sqac\", task=\"question-answering\")\n",
        "display(pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ec686c9",
      "metadata": {
        "id": "1ec686c9"
      },
      "source": [
        "Vamos a tomar un texto donde se habla de Gabriel Garcia Marquez:\n",
        "> Gabriel José de la Concordia García Márquez (Aracataca, Colombia, 6 de marzo de 1927-Ciudad de México, 17 de abril de 2014) fue un escritor y periodista colombiano. Reconocido principalmente por sus novelas y cuentos, también escribió narrativa de no ficción, discursos, reportajes, críticas cinematográficas y memorias. Estudió derecho y periodismo en la Universidad Nacional de Colombia, e inicio sus primeras colaboraciones periodísticas en el diario 'El Espectador'. Fue conocido como Gabo, y familiarmente y por sus amigos, como Gabito. En 1982 recibió el Premio Nobel de Literatura «por sus novelas e historias cortas, en las que lo fantástico y lo real se combinan en un mundo ricamente compuesto de imaginación, lo que refleja la vida y los conflictos de un continente»"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84602983",
      "metadata": {
        "title": "[code]",
        "id": "84602983"
      },
      "outputs": [],
      "source": [
        "context = \"Gabriel José de la Concordia García Márquez (Aracataca, Colombia, 6 de marzo de 1927-Ciudad de México, 17 de abril de 2014) fue un escritor y periodista colombiano. Reconocido principalmente por sus novelas y cuentos, también escribió narrativa de no ficción, discursos, reportajes, críticas cinematográficas y memorias. Estudió derecho y periodismo en la Universidad Nacional de Colombia, e inicio sus primeras colaboraciones periodísticas en el diario 'El Espectador'. Fue conocido como Gabo, y familiarmente y por sus amigos, como Gabito. En 1982 recibió el Premio Nobel de Literatura «por sus novelas e historias cortas, en las que lo fantástico y lo real se combinan en un mundo ricamente compuesto de imaginación, lo que refleja la vida y los conflictos de un continente»\"\n",
        "pprint(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76f5feea",
      "metadata": {
        "id": "76f5feea"
      },
      "source": [
        "Ahora, planteamos la pregunta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc1c1cd",
      "metadata": {
        "title": "[code]",
        "id": "edc1c1cd"
      },
      "outputs": [],
      "source": [
        "question = \"¿Quién ganó un premio Nobel?\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "817fb889",
      "metadata": {
        "id": "817fb889"
      },
      "source": [
        "Veamos la respuesta del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35872ce1",
      "metadata": {
        "title": "[code]",
        "id": "35872ce1"
      },
      "outputs": [],
      "source": [
        "res = pipe(\n",
        "    question=question,\n",
        "    context=context,\n",
        ")\n",
        "display(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d8303f0",
      "metadata": {
        "id": "2d8303f0"
      },
      "source": [
        "Veamos un ejemplo con otra pregunta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c219dd5",
      "metadata": {
        "title": "[code]",
        "id": "4c219dd5"
      },
      "outputs": [],
      "source": [
        "question = \"¿Dónde estudió Gabo?\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4bf0f15",
      "metadata": {
        "id": "e4bf0f15"
      },
      "source": [
        "Veamos la respuesta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0178bff6",
      "metadata": {
        "title": "[code]",
        "id": "0178bff6"
      },
      "outputs": [],
      "source": [
        "res = pipe(\n",
        "    question=question,\n",
        "    context=context,\n",
        ")\n",
        "display(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1258d0e5",
      "metadata": {
        "id": "1258d0e5"
      },
      "source": [
        "### **4.8. Zero-Shot Classification**\n",
        "---\n",
        "\n",
        "Zero-shot classification con LLMs (Language Model for Zero-shot Classification) es una técnica que combina el poder de los modelos de lenguaje basados en redes neuronales con la capacidad de realizar clasificación sin etiquetas. En lugar de entrenar el modelo en un conjunto de datos etiquetados para categorías específicas, se utiliza el conocimiento previo del LLM para asignar automáticamente etiquetas a nuevas categorías sin entrenamiento explícito. Esto se logra mediante la representación de las categorías y los datos de entrada en un espacio semántico común, donde el modelo puede realizar inferencias basadas en la proximidad y las relaciones semánticas entre ellos. Al aprovechar esta información, el LLM puede asignar etiquetas a nuevas instancias de datos sin necesidad de ejemplos de entrenamiento específicos. Esta capacidad de zero-shot classification con LLMs permite una clasificación más flexible y escalable, donde el modelo puede adaptarse a nuevas categorías sin requerir una nueva fase de entrenamiento. Esta técnica ha demostrado ser útil en diversas tareas de procesamiento del lenguaje natural y clasificación de texto, y ha abierto nuevas posibilidades en la capacidad de los modelos de comprender y clasificar datos en un contexto más amplio y sin restricciones.\n",
        "\n",
        "<center> <img src=\"https://drive.google.com/uc?export=view&id=1qTc1duv30tfQXBy34do01jDn0wpk7-be\" width=\"60%\"> </center>\n",
        "\n",
        "Veamos un ejemplo de zero-shot classification con el modelo `bert` en español. Comenzamos cargando el `pipeline` para la tarea `zero-shot-classification`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d069554c",
      "metadata": {
        "title": "[code]",
        "id": "d069554c"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    model=\"Recognai/bert-base-spanish-wwm-cased-xnli\",\n",
        "    task=\"zero-shot-classification\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aad8079",
      "metadata": {
        "id": "5aad8079"
      },
      "source": [
        "Ahora, vamos a definir unas posibles categorías a clasificar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d69454",
      "metadata": {
        "title": "[code]",
        "id": "38d69454"
      },
      "outputs": [],
      "source": [
        "labels = [\"computación\", \"filosofía\", \"artes\", \"arquitectura\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d382d19",
      "metadata": {
        "id": "8d382d19"
      },
      "source": [
        "Veamos cómo clasifica el modelo en estas categorías:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ab7de4b",
      "metadata": {
        "title": "[code]",
        "id": "2ab7de4b"
      },
      "outputs": [],
      "source": [
        "res = pipe(\n",
        "    \"El algoritmo de la transformada rápida de Fourier es uno de los mas importantes de la historia\",\n",
        "    candidate_labels=labels,\n",
        ")\n",
        "display(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c056af8",
      "metadata": {
        "id": "0c056af8"
      },
      "source": [
        "### **4.9. Generación de Texto**\n",
        "---\n",
        "\n",
        "La generación de texto con LLMs (Language Model for Text Generation) se refiere al proceso de utilizar modelos de lenguaje basados en redes neuronales para producir texto coherente y relevante automáticamente. Estos modelos se entrenan en grandes conjuntos de datos de texto,  aprenden a capturar las estructuras gramaticales, las relaciones semánticas y el estilo del lenguaje. Cuando se les proporciona una semilla o un contexto inicial, los LLMs pueden generar texto secuencialmente, palabra por palabra, utilizando su conocimiento previo y su capacidad para predecir las probabilidades de ocurrencia de las palabras en función del contexto. Esto permite la generación de texto en una variedad de aplicaciones, como la redacción automática, la creación de diálogos, la traducción automática y la generación de resúmenes. La generación de texto con LLMs ha avanzado significativamente en los últimos años y ha logrado producir resultados sorprendentemente realistas y coherentes, aunque todavía existen desafíos en la calidad y la diversidad de las generaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73736f32",
      "metadata": {
        "title": "[code]",
        "id": "73736f32",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @markdown ##**Ejecute esta celda para ver el video.**\n",
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(\n",
        "    src=\"https://drive.google.com/uc?export=view&id=1OgRlHlDPddWcCmxLIO0Q_mIBaPZppbRZ\",\n",
        "    width=\"768px\",\n",
        "    height=\"432px\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f16e34ba",
      "metadata": {
        "id": "f16e34ba"
      },
      "source": [
        "Veamos un ejemplo con un modelo `gpt2` para la generación de poemas en español. Comenzamos descargando el modelo con la tarea `text-generation`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ba225b2",
      "metadata": {
        "title": "[code]",
        "id": "2ba225b2"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(model=\"ismaelfaro/gpt2-poems.es\", task=\"text-generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab4975dd",
      "metadata": {
        "id": "ab4975dd"
      },
      "source": [
        "Veamos un ejemplo de generación, especificando que queremos generar 100 palabras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0faa57f8",
      "metadata": {
        "title": "[code]",
        "id": "0faa57f8"
      },
      "outputs": [],
      "source": [
        "res = pipe(\"En el sueño\", max_new_tokens=100)\n",
        "display(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a571007d",
      "metadata": {
        "id": "a571007d"
      },
      "source": [
        "## **Recursos Adicionales**\n",
        "---\n",
        "\n",
        "Los siguentes enlaces corresponden a las sitios donde encontrará información util para profundizar en los temas vistos en este taller guiado:\n",
        "\n",
        "- [Transformers documentation](https://huggingface.co/docs/transformers/index).\n",
        "- [Datasets documentation](https://huggingface.co/docs/datasets/index).\n",
        "- [Tokenizers documentation](https://huggingface.co/docs/tokenizers/index).\n",
        "- [Hub documentation](https://huggingface.co/docs/hub/index).\n",
        "- [Tasks documentation](https://huggingface.co/tasks).\n",
        "- [How to add a pipeline to transformers?](https://huggingface.co/docs/transformers/add_new_pipeline)"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "title,-all",
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}