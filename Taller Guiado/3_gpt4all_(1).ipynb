{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8142c196",
      "metadata": {
        "id": "8142c196"
      },
      "source": [
        "# Gpt4All\n",
        "---\n",
        "\n",
        "`gpt4all` es una herramienta de código abierto que permite desplegar grandes modelos del lenguaje (LLMs) en cualquier hardware. Una de las mayores ventajas es que podemos utilizar modelos de hasta 13 miles de millones de parámetros en un computador personal o de escritorio sin la necesidad de tener una unidad de procesamiento gráfico (GPU - Graphical Processing Unit) y realizando la inferencia en un procesador convencional (CPU).\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1bPqMcRD1sylQOppygNakQBx5p0Ewf2TZ\" width=\"20%\"></center>\n",
        "\n",
        "Esta herramienta surge como una alternativa a:\n",
        "\n",
        "- Herramientas que no dan un acceso directo a los modelos como OpenAI.\n",
        "- Herramientas que requieren muchos recursos computacionales como tarjetas gráficas de mucha potencia.\n",
        "\n",
        "Veamos cómo podemos realizar la instalación desde _Python_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c627e7d",
      "metadata": {
        "title": "[code]",
        "id": "6c627e7d"
      },
      "outputs": [],
      "source": [
        "!pip install gpt4all"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1ad4b74",
      "metadata": {
        "id": "a1ad4b74"
      },
      "source": [
        "Adicionalmente, utilizaremos algunas utilidades generales:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c176f03d",
      "metadata": {
        "title": "[code]",
        "id": "c176f03d"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a7b09c5",
      "metadata": {
        "id": "9a7b09c5"
      },
      "source": [
        "## **1. Búsqueda de Modelos**\n",
        "---\n",
        "\n",
        "`gpt4all` utiliza distintas estrategias para permitir utilizar modelos en CPU de forma eficiente. Veamos algunas de estas:\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1iF3trBjj4cWdmzbdNZx1fJRu6_-DtFy_\" width=\"50%\"></center>\n",
        "\n",
        "- **Modelos pequeños**: en `gpt4all` disponemos de modelos que no son tan masivos como los que ofrece OpenAI. Por ejemplo, el modelo detrás de ChatGPT (GPT-3.5) utiliza 175 billones de parámetros. En cambio, los modelos disponibles tienen entre 3 y 13 billones de parámetros.\n",
        "- **Cuantificación**: los modelos de `gpt4all` realizan inferencia con técnicas de cuantificación, la cual trata de una técnica computacional que permite comprimir datos al reducir la precisión numérica. Por ejemplo, hay modelos que utilizan una versión cuantificada a 4 bits del modelo. Esto difiere de los modelos convencionales que trabajan con precisiones de 32 o 64 bits, lo que significa en una reducción de alrededor de 8 hasta 16 veces del uso de memoria de los modelos.\n",
        "- **Optimización**: los modelos de `gpt4all` vienen listos para su uso en inferencia. Internamente fueron implementados en lenguajes de bajo nivel como C/C++ lo que permite que se ejecuten de una forma bastante eficiente y no requieren un framework intermedio como `pytorch` o `tensorflow`.\n",
        "\n",
        "Para poder utilizar un modelo de `gpt4all` debemos identificar principalmente dos cosas:\n",
        "\n",
        "- Tamaño del modelo.\n",
        "- Conjunto de datos donde fue entrenado.\n",
        "\n",
        "Esto lo podemos encontrar en la descripción en la página [oficial](https://gpt4all.io/index.html) como se puede observar en el siguiente video:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a73b47a2",
      "metadata": {
        "id": "a73b47a2",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##**Ejecute esta celda para ver el video.**\n",
        "from IPython.display import IFrame\n",
        "IFrame(\n",
        "        src=\"https://drive.google.com/file/d/1CaJX9IJccJnygoA5EqhwAwfSsXHUWSw1/preview\",\n",
        "        width=\"768px\",\n",
        "        height=\"432px\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07c79432",
      "metadata": {
        "id": "07c79432"
      },
      "source": [
        "Vamos a descargar un modelo `falcon` con cuantificación a 4 bits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d031917f",
      "metadata": {
        "title": "[code]",
        "id": "d031917f"
      },
      "outputs": [],
      "source": [
        "# ![[ -d mlds7_models/ ]] && rm -rf mlds7_models\n",
        "!git clone https://huggingface.co/juselara1/mlds7_models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8d94910",
      "metadata": {
        "id": "a8d94910"
      },
      "source": [
        "Como podemos ver, el modelo se descarga con el nombre `ggml-model-gpt4all-falcon-q4_0.bin`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66fe0165",
      "metadata": {
        "title": "[code]",
        "id": "66fe0165"
      },
      "outputs": [],
      "source": [
        "!ls mlds7_models/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5645997",
      "metadata": {
        "id": "e5645997"
      },
      "source": [
        "## **2. Carga de Modelos**\n",
        "---\n",
        "\n",
        "Para cargar un modelo, debemos importar la clase `GPT4ALL`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33740942",
      "metadata": {
        "title": "[code]",
        "id": "33740942"
      },
      "outputs": [],
      "source": [
        "from gpt4all import GPT4All"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "324243ef",
      "metadata": {
        "id": "324243ef"
      },
      "source": [
        "Para cargar el modelo, debemos indicar el nombre del archivo descargado:\n",
        "\n",
        "> **Nota**: este modelo está cargado y utilizará los recursos del equipo donde estemos ejecutando este código. Esto es diferente de OpenAI donde la ejecución se realiza en un servidor de la misma empresa y únicamente tenemos acceso al resultado final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9797761d",
      "metadata": {
        "title": "[code]",
        "id": "9797761d"
      },
      "outputs": [],
      "source": [
        "model = GPT4All(\n",
        "        model_name=\"ggml-model-gpt4all-falcon-q4_0.bin\",\n",
        "        model_path=\"mlds7_models/\"\n",
        "        )\n",
        "display(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798df90b",
      "metadata": {
        "id": "798df90b"
      },
      "source": [
        "Es importante resaltar que `gpt4all` está pensado fundamentalmente para inferencia. Veamos una comparativa de las tres herramientas que estamos presentando:\n",
        "\n",
        "| Aspecto | HuggingFace | OpenAI | GPT4All |\n",
        "| --- | --- | --- | --- |\n",
        "| Entrenamiento | ✔ | ✔ | ✗ |\n",
        "| Modificación de modelos | ✔ | ✗ | ✗ |\n",
        "| Inferencia rápida | ✗ | ✔ | ✔ |\n",
        "| Código abierto | ✔ | ✗ | ✔ |\n",
        "\n",
        "Es importante resaltar que aunque OpenAI permite entrenamiento de modelos, nunca tendremos acceso a estos y únicamente podemos utilizarlos para inferencia tal y como los modelos que ya existen en esta plataforma.\n",
        "\n",
        "Con `gpt4all`, principalmente estaremos realizando dos tareas: **generación de texto** y **sesiones de chat**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84b797fe",
      "metadata": {
        "id": "84b797fe"
      },
      "source": [
        "## **3. Generación de Texto**\n",
        "---\n",
        "\n",
        "Cualquiera de los modelos de `gpt4all` puede usarse para la generación de texto a partir de una cadena original.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1rOfivYF0dQI9vvwA-etx1wsKxd0B9QEL\" width=\"90%\"></center>\n",
        "\n",
        "Veamos cómo funciona esto para obtener una descripción de Alan Turing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be4835fa",
      "metadata": {
        "title": "[code]",
        "id": "be4835fa"
      },
      "outputs": [],
      "source": [
        "text = \"Alan Turing was \""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e98e27c1",
      "metadata": {
        "id": "e98e27c1"
      },
      "source": [
        "Ahora, utilizamos el modelo para generar 10 tokens (determinado por el parámetro `max_tokens`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9f91bfa",
      "metadata": {
        "title": "[code]",
        "id": "f9f91bfa"
      },
      "outputs": [],
      "source": [
        "output = model.generate(text, max_tokens=30)\n",
        "display(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bcab4d1",
      "metadata": {
        "id": "7bcab4d1"
      },
      "source": [
        "El método `generate` permite utilizar los siguientes hiperparámetros:\n",
        "\n",
        "- `max_tokens`: máximo número de tokens a generar.\n",
        "- `temp`: temperatura para la predicción, son valores positivos en donde, entre más alto sea este valor, las respuestas son menos deterministas.\n",
        "- `top_k`: específica un tamaño para seleccionar los tokens más probables a seleccionar en cada paso de la generación.\n",
        "- `top_p`: equivalente a `top_k` con la diferencia que este hiperparámetro señala un valor de probabilidad acumulada que deben tener los tokens de la muestra.\n",
        "- `repeat_penalty`: penalización al modelo por repetitividad.\n",
        "- `repeat_last_n`: indica qué tantas palabras previas son consideradas para evaluar la repetitividad.\n",
        "- `n_batch`: número de tokens que se procesan en paralelo.\n",
        "- `streaming`: específica si el resultado se puede utilizar como un generador.\n",
        "\n",
        "Veamos un ejemplo con un valor de temperatura mayor (menos determinístico), esto puede llevar al efecto de **alucinación** (inventarse cosas que parecen verdaderas):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8d6fd0",
      "metadata": {
        "title": "[code]",
        "id": "7b8d6fd0"
      },
      "outputs": [],
      "source": [
        "output = model.generate(text, temp=100.0, max_tokens=30)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1ae66c0",
      "metadata": {
        "id": "d1ae66c0"
      },
      "source": [
        "Veamos un ejemplo con un valor de temperatura más bajo (más determinístico, obtenemos respuestas parecidas cada vez que generamos texto):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7343aa61",
      "metadata": {
        "title": "[code]",
        "id": "7343aa61"
      },
      "outputs": [],
      "source": [
        "output = model.generate(text, temp=0.1, max_tokens=30)\n",
        "display(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35815b30",
      "metadata": {
        "id": "35815b30"
      },
      "source": [
        "## **4. Sesiones de Chat**\n",
        "---\n",
        "\n",
        "`gpt4all` permite implementar de forma sencilla conversaciones por medio de las sesiones de chat. En especial, las sesiones de chat cumplen dos roles:\n",
        "\n",
        "1. Al modelo se le asigna una plantilla (template) para que la generación sirva como un chat.\n",
        "2. Salidas intermedias del modelo se almacenan en un cache para que el histórico de la conversación se representa directamente. Esto acelera la inferencia.\n",
        "\n",
        "La sesión de chat se crea con el método `chat_session` dentro de un contexto. Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d67bcede",
      "metadata": {
        "title": "[code]",
        "id": "d67bcede"
      },
      "outputs": [],
      "source": [
        "with model.chat_session():\n",
        "    response = model.generate(\"Hello\", top_k=1)\n",
        "    response = model.generate(\"Who invented the light bulb?\", top_k=1)\n",
        "    response = model.generate(\"Thanks\", top_k=1)\n",
        "    display(model.current_chat_session)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c4f01ae",
      "metadata": {
        "id": "0c4f01ae"
      },
      "source": [
        "## **Recursos Adicionales**\n",
        "---\n",
        "\n",
        "Los siguientes enlaces corresponden a las sitios donde encontrará información útil para profundizar en los temas vistos en este taller guiado:\n",
        "\n",
        "- [GPT4All in Python](https://docs.gpt4all.io/gpt4all_python.html#quickstart).\n",
        "- [GPT4All: Running an Open-source ChatGPT Clone on Your Laptop](https://betterprogramming.pub/gpt4all-running-an-open-source-chatgpt-clone-on-your-laptop-71ebe8600c71)."
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "title,-all",
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}